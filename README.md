# API Reverse Engineering Tool

Full-stack application for reverse-engineering APIs from HAR files using LLM-powered analysis.

## Features

- **HAR File Upload & Inspection**: Upload `.har` files and browse individual requests
- **LLM-Powered Matching**: Describe an API endpoint and get the best-matching request
- **PII & Secret Redaction**: Automatic sanitization of sensitive data before LLM processing
- **Token-Efficient Processing**: Two-pass strategy for large HAR files
- **Deterministic curl Generation**: curl is built from the matched HAR entry (not generated by the LLM)
- **Interactive Request Editor**: Edit method, URL, headers, query params, and body before executing
- **Server-side Request Execution + Response Viewer**: Execute requests safely and inspect status/headers/body/duration

## Tech Stack

- **Frontend**: Next.js (App Router) + shadcn/ui + Tailwind CSS
- **Backend**: NestJS + OpenAI SDK + Axios
- **Architecture**: BFF (Backend-for-Frontend) pattern

## Monorepo Structure

```
reverse-api/
├── frontend/          # Next.js application
│   ├── src/
│   │   ├── app/       # App Router pages & layouts
│   │   ├── components/# UI components
│   │   └── lib/       # API client, types, utilities
│   └── package.json
├── backend/           # NestJS application
│   ├── src/
│   │   ├── har/       # HAR upload & parsing
│   │   ├── llm/       # LLM orchestration & redaction
│   │   ├── curl/      # Curl execution endpoint + service
│   │   └── shared/    # Config, filters, guards, session store
│   └── package.json
├── .env.example       # Environment variables template
├── .env.secrets.example
└── README.md
```

## Getting Started

### Prerequisites

- Node.js 18+
- OpenAI API key

### Installation

1. Clone the repository
2. Install dependencies:
   ```bash
   # Backend
   cd backend && npm install
   
   # Frontend
   cd ../frontend && npm install
   ```

3. Set up environment variables:
   
   **Configuration (`.env`)**:
   ```bash
   cp .env.example .env
   # Review and adjust configuration values as needed
   ```
   
   **Secrets (`.env.secrets`)** - ⚠️ Restricted Access:
   ```bash
   cp .env.secrets.example .env.secrets
   # Add your OPENAI_API_KEY and other sensitive credentials
   # DO NOT share or commit this file
   ```

### Environment Files

This project uses two separate environment files for security:

| File | Purpose | Access Level | Commit to Git? |
|------|---------|--------------|----------------|
| `.env` | Non-sensitive configuration (ports, limits, thresholds) | Team-wide | ❌ No (varies by environment) |
| `.env.secrets` | Sensitive credentials (API keys, tokens, passwords) | **Restricted** | ❌ **NEVER** |
| `.env.example` | Configuration template | Public | ✅ Yes |
| `.env.secrets.example` | Secrets template | Public | ✅ Yes |

**Production Best Practices:**
- Use secret management tools: AWS Secrets Manager, HashiCorp Vault, Google Secret Manager
- Rotate API keys regularly
- Implement role-based access control (RBAC) for secrets
- Audit secret access logs

### Running the Application

1. Start the backend (port 3001):
   ```bash
   cd backend
   npm run start:dev
   ```

2. Start the frontend (port 3000):
   ```bash
   cd frontend
   npm run dev
   ```

3. Open http://localhost:3000 in your browser

## Configuration

All configurable parameters are defined in `.env.example`. Key settings:

### Backend Configuration
- `OPENAI_API_KEY` (required): Your OpenAI API key (in `.env.secrets`)
- `OPENAI_MODEL` (required): OpenAI model to use (in `.env.secrets`)
- `LLM_SINGLE_PASS_THRESHOLD` (default: 15): Max entries for single-pass LLM strategy
- `LLM_TOP_K_CANDIDATES` (default: 3): Number of candidates selected in Pass 1
- `LLM_MAX_RETRIES` (default: 2): Retries on malformed/invalid model output
- `MAX_HAR_ENTRIES` (default: 500): Maximum HAR entries to process
- `MAX_HAR_SIZE_MB` (default: 100): Maximum HAR file size in MB (max: 200)
- `BODY_TRUNCATE_LIMIT` (default: 10000): Max characters for request/response body previews sent to the LLM
- `CURL_EXEC_TIMEOUT_MS` (default: 10000): Timeout for server-side execution via `POST /curl/execute`
- `CURL_EXEC_MAX_RESPONSE_MB` (default: 1): Max upstream response body size (MB)
- `CURL_EXEC_MAX_REQUEST_BODY_MB` (default: 1): Max outbound request body size (MB)
- `SESSION_STORE_TTL_MINUTES` (default: 30): Session expiration time
- `THROTTLE_TTL` / `THROTTLE_LIMIT`: Rate limiting window + max requests per window (applies to API routes)
- `PORT` (default: 3001): Backend server port

### Frontend Configuration
- `BACKEND_URL` (default: http://localhost:3001): Backend API URL for Next.js rewrites
  - **Development**: `http://localhost:3001`
  - **Production**: Set to your deployed backend URL (e.g., `https://api.yourdomain.com`)

## Session Store Scalability

The current implementation uses an in-memory `Map` for session storage. This is suitable for:
- Single-instance deployments
- Development and testing
- Demo/proof-of-concept scenarios

**For production horizontal scaling**, replace the in-memory store with Redis:
- Install `@nestjs/redis` or `ioredis`
- Implement a `RedisSessionStore` service
- Update `SessionStoreService` to use Redis
- Configure Redis connection in `.env`

## Security Features

- **PII/Secret Redaction for LLM**: Pass 1 uses sanitized URLs (no query, normalized high-entropy path segments); Pass 2 redacts header values (allowlist) and body previews
- **Input Validation**: DTO validation + global `ValidationPipe` (`whitelist`, `forbidNonWhitelisted`, `transform`)
- **Rate Limiting**: Throttling on API routes
- **Secure Headers**: Helmet middleware for CSP, HSTS, etc.
- **File Size Limits**: Configurable max HAR file size (default: 100MB, max: 200MB)
- **Session Management**: Time-limited temporary storage with automatic cleanup
- **No logging of secrets**: Avoid logging request headers/bodies/curl strings (treat analyze results as sensitive)

### Notes on SSRF

`POST /curl/execute` is protected by an SSRF guard that blocks private/internal network targets (for example: `localhost`, RFC1918 ranges, link-local metadata IPs) and only allows `http://` and `https://` URLs.

Redirects are disabled during execution (`maxRedirects: 0`). This prevents SSRF bypass via `Location` redirects.

Known limitation: DNS rebinding is not fully preventable at the application layer; production deployments should also use network-level egress controls.

## Architecture

### API Proxy Pattern

The frontend uses Next.js rewrites to proxy all `/api/*` requests to the backend:
- **Frontend requests**: `/api/har/upload`
- **Proxied to backend**: `${BACKEND_URL}/har/upload`

This eliminates CORS issues and provides a single origin for the application.

### Backend Endpoints (NestJS)

- `POST /har/upload`: Upload and parse HAR file
- `POST /har/analyze`: LLM analysis to match API description
- `GET /har/sessions/:sessionId/entries/:index`: Fetch a full HAR entry for inspection
- `POST /curl/execute`: Execute an edited `ParsedRequest` and return `{ status, statusText, headers, body, duration }`
  - Target non-2xx responses (4xx/5xx) are returned as normal results; only connection-level failures/SSRF/validation return API errors.

## Deployment

### Environment Variables

**Backend (`.env` + `.env.secrets` at repo root)**:

Configuration (`.env`):
```bash
PORT=3001
LLM_SINGLE_PASS_THRESHOLD=15
# ... other configuration
```

Secrets (`.env.secrets`):
```bash
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=your_model_here
```

**Frontend (`frontend/.env.local` or deployment platform)**:
```bash
# Point to your deployed backend
BACKEND_URL=https://api.yourdomain.com
```

### Deployment Notes

1. **Backend**: Deploy to any Node.js hosting (Heroku, Railway, AWS, etc.)
2. **Frontend**: Deploy to Vercel, Netlify, or any Next.js-compatible platform
3. **Set `BACKEND_URL`** on your frontend deployment to point to the deployed backend
4. **No CORS configuration needed** - the proxy handles cross-origin requests

### Production hardening checklist (recommended)

- Replace in-memory session store with Redis
- Add network-level egress controls (VPC egress, firewall rules) to further mitigate SSRF/DNS rebinding
- Add auth to the app itself (even basic) if exposed publicly
- Add structured logging + error monitoring (ensure no secrets are emitted)

## Testing

```bash
# Backend unit tests
cd backend
npm run test

# Backend e2e tests
npm run test:e2e

# Frontend build (typecheck + production build)
cd ../frontend
npm run build

# Coverage
cd ../backend
npm run test:cov
```

### Manual end-to-end testing checklist (UI)

1. Upload a HAR file.
2. Describe an API and run Analyze to generate curl.
3. Click **Edit and Run** and confirm the editor is pre-populated.
4. Execute against a reachable URL and confirm the response viewer shows:
   - status badge
   - headers
   - body
   - duration
5. Execute against an endpoint that returns 404/401 and confirm it renders as a normal response (not a toast error).
6. Execute against a blocked SSRF target (for example `http://169.254.169.254/`) and confirm it shows a toast error and does not render a response result.

## License

MIT
